{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"DwtsAeDBFXLm","executionInfo":{"status":"ok","timestamp":1705394358545,"user_tz":480,"elapsed":6737,"user":{"displayName":"Aryan Trehan (Lord_Breadcrumb)","userId":"18395214618401003016"}}},"outputs":[],"source":["import torch\n","from torch.utils.data import DataLoader\n","from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n","from tqdm import tqdm\n","import pandas as pd\n","import numpy as np\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","source":["num_classes = 3\n","\n","model_name = \"bert-base-uncased\"\n","tokenizer = BertTokenizer.from_pretrained(model_name)\n","model = BertForSequenceClassification.from_pretrained(model_name, num_labels=num_classes)\n","model.to(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ni_cpIAOI1rB","executionInfo":{"status":"ok","timestamp":1705394362279,"user_tz":480,"elapsed":3738,"user":{"displayName":"Aryan Trehan (Lord_Breadcrumb)","userId":"18395214618401003016"}},"outputId":"2c9b6e1f-fb87-4713-a47e-1a80e3bd0aa9"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"execute_result","data":{"text/plain":["BertForSequenceClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",")"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["from sklearn.preprocessing import OneHotEncoder\n","from torch.utils.data import Dataset\n","from sklearn.preprocessing import OneHotEncoder\n","from sklearn.model_selection import train_test_split\n","from torch.nn.utils.rnn import pad_sequence\n","\n","\n","class CustomDataset(Dataset):\n","    def __init__(self, texts, labels, tokenizer, max_len=512):\n","        self.texts = texts\n","        self.labels = labels\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def __getitem__(self, idx):\n","        text = str(self.texts[idx])\n","        label = self.labels[idx]\n","\n","        encoding = self.tokenizer.encode_plus(\n","            text,\n","            add_special_tokens=True,\n","            max_length=self.max_len,\n","            return_token_type_ids=False,\n","            padding='max_length',\n","            return_attention_mask=True,\n","            return_tensors='pt',\n","            truncation=True\n","        )\n","\n","        return {\n","            'input_ids': encoding['input_ids'].flatten(),\n","            'attention_mask': encoding['attention_mask'].flatten(),\n","            'label': torch.tensor(label, dtype=torch.float32)\n","        }\n","\n","data = pd.read_csv('train.csv')\n","X_train = np.array(data['Comment'])\n","y_train = np.array(data['Topic'])\n","\n","data = pd.read_csv('test.csv')\n","X_test = np.array(data['Comment'])\n","y_test = np.array(data['Topic'])\n","\n","onehot_en = OneHotEncoder(sparse_output=False)\n","y_train = onehot_en.fit_transform(y_train.reshape(-1,1))\n","y_test = onehot_en.fit_transform(y_test.reshape(-1,1))\n","\n","train_dataset = CustomDataset(X_train,y_train,tokenizer)\n","test_dataset = CustomDataset(X_test, y_test,tokenizer)\n","\n","batch_size = 16\n","\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"],"metadata":{"id":"5PXIYscAJfye","executionInfo":{"status":"ok","timestamp":1705394363035,"user_tz":480,"elapsed":768,"user":{"displayName":"Aryan Trehan (Lord_Breadcrumb)","userId":"18395214618401003016"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["import torch\n","from torch.utils.data import DataLoader\n","from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n","from tqdm import tqdm\n","\n","learning_rate = 0.0001\n","\n","optimizer = AdamW(model.parameters(), lr=learning_rate)\n","\n","num_epochs = 3\n","for epoch in range(num_epochs):\n","    model.train()\n","    total_loss = 0.0\n","\n","    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n","        input_ids = batch[\"input_ids\"].to(device)\n","        attention_mask = batch[\"attention_mask\"].to(device)\n","        labels = batch[\"label\"].to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs.loss\n","        total_loss += loss.item()\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","    average_loss = total_loss / len(train_loader)\n","    print(f\"Epoch {epoch+1}, Average Loss: {average_loss}\")\n","\n","    # Validation loop\n","    model.eval()\n","    with torch.no_grad():\n","        correct = 0\n","        total = 0\n","\n","        for batch in tqdm(test_loader, desc=\"Validation\"):\n","            input_ids = batch[\"input_ids\"].to(device)\n","            attention_mask = batch[\"attention_mask\"].to(device)\n","            labels = batch[\"label\"].to(device)\n","\n","            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","            logits = outputs.logits\n","\n","            _, predicted = torch.max(logits, 1)\n","            total += labels.size(0)\n","            _, labs = torch.max(labels, 1)\n","            for i in range(len(predicted)):\n","              if predicted[i] == labs[i]:\n","                correct += 1\n","\n","        accuracy = correct / total\n","        print(f\"Validation Accuracy: {accuracy}\")\n","\n","# Save the fine-tuned model\n","torch.save(model.state_dict(), \"fine_tuned_bert_model.pth\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4QXrGvxjUYV7","outputId":"57ae4095-a0f7-42aa-80f2-59a79771fc0d","executionInfo":{"status":"ok","timestamp":1705396792798,"user_tz":480,"elapsed":2429766,"user":{"displayName":"Aryan Trehan (Lord_Breadcrumb)","userId":"18395214618401003016"}}},"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","Epoch 1: 100%|██████████| 544/544 [12:35<00:00,  1.39s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1, Average Loss: 0.4360759374137749\n"]},{"output_type":"stream","name":"stderr","text":["Validation: 100%|██████████| 100/100 [00:55<00:00,  1.82it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Validation Accuracy: 0.8291298865069356\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 2: 100%|██████████| 544/544 [12:34<00:00,  1.39s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2, Average Loss: 0.31252703408245\n"]},{"output_type":"stream","name":"stderr","text":["Validation: 100%|██████████| 100/100 [00:54<00:00,  1.83it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Validation Accuracy: 0.8530895334174022\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 3: 100%|██████████| 544/544 [12:32<00:00,  1.38s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 3, Average Loss: 0.23269713845665513\n"]},{"output_type":"stream","name":"stderr","text":["Validation: 100%|██████████| 100/100 [00:54<00:00,  1.83it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Validation Accuracy: 0.8562421185372006\n"]}]}]}